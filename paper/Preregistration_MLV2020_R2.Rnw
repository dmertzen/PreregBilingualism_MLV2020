\documentclass[doc,floatsintext,12pt]{apa6}
%\documentclass[man]{apa6}

\usepackage{graphicx}

\usepackage[american]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
%% customize as needed:
\usepackage[backend=biber,style=apa]{biblatex}

\usepackage{fancyvrb}


\usepackage{url}
\usepackage[colorlinks=true,
            linkcolor=blue,
            urlcolor=blue,
            citecolor=black]{hyperref}

\usepackage{lscape} % landscape table
\usepackage{longtable}
\usepackage{threeparttablex}
\usepackage{booktabs}
\usepackage{multirow} % multirows in tables 
\usepackage{bigdelim} % curly braces in table
\usepackage{xcolor,colortbl}

\usepackage[export]{adjustbox}
\usepackage{caption, subcaption, floatrow}

\usepackage{mathtools}
\makeatletter
 
\newcommand{\explain}[2]{\underset{\mathclap{\overset{\uparrow}{#2}}}{#1}}
\newcommand{\explainup}[2]{\overset{\mathclap{\underset{\downarrow}{#2}}}{#1}}
 
\makeatother

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\newtheorem{definition}{Definition}[section]

\usepackage{microtype}
\usepackage[american]{babel}
\usepackage[utf8]{inputenc}
\usepackage{csquotes}
\usepackage{setspace}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{lineno,clipboard}

\newclipboard{reviews}


\usepackage[most]{tcolorbox}

%% for revisions
\newcommand{\revised}[1]{{\color{black}{#1}}}
%\newcommand{\revised}[1]{{\color{blue}{#1}}}


%\usepackage{tikz}
\usepackage{todonotes}

\usepackage{gb4e}


\DeclareLanguageMapping{american}{american-apa}
\addbibresource{bibcleaned.bib}

\AtEveryBibitem{\clearfield{issue}}


\leftheader{Mertzen,Lago,Vasishth}
\title{The benefits of preregistration for hypothesis-driven bilingualism research}
\shorttitle{Preregistration in bilingualism language research}

\threeauthors{Daniela Mertzen}{Sol Lago}{Shravan Vasishth}

\threeaffiliations{Department of Linguistics, University of Potsdam, Potsdam, Germany}{Institute for Romance Languages and Literatures, Goethe University Frankfurt, Frankfurt, Germany}{Department of Linguistics, University of Potsdam, Potsdam, Germany}

\authornote{Correspondence: mertzen@uni-potsdam.de. All data and code are available from \url{https://osf.io/5ab7d/}.}
\note{\today}

\journal{Submitted to Bilingualism: Language and Cognition} 
\volume{} 

\keywords{preregistration; open science; bilingualism; psycholinguistics; confirmatory analysis; exploratory analysis}


\abstract{\Copy{clarify1}{\revised{Preregistration is an open science practice that requires the specification of research hypotheses and analysis plans \textit{before} the data are inspected.}}\label{pageclarify1} Here, we discuss the benefits of preregistration for hypothesis-driven, confirmatory bilingualism research. 
Using examples from psycholinguistics and bilingualism, we illustrate how non-peer reviewed preregistrations can serve to implement a clean distinction between hypothesis testing and data exploration. This distinction helps researchers avoid casting post-hoc hypotheses and analyses as confirmatory ones. We argue that, in keeping with current best practices in the experimental sciences, preregistration, along with sharing data and code, should be an integral part of hypothesis-driven bilingualism research.}

\begin{document}


\maketitle

<<setup,include=FALSE,cache=FALSE,echo=FALSE>>=
library(MASS)
library(knitr)
library(xtable)
#library(papaja)
library(tidyverse)
library(rstan)
options(mc.cores = parallel::detectCores())
library(brms)
library(gridExtra)
library(bayesplot)
library(ggridges)
library(lme4)
library(reshape2)

#theme_set(theme_apa())

# set global chunk options, put figures into folder
options(warn=-1, replace.assign=TRUE)
opts_chunk$set(fig.path='figures/figure-', fig.align='center', fig.show='hold')
options(replace.assign=TRUE,width=75)
opts_chunk$set(dev='postscript')
opts_chunk$set(echo = TRUE)
@


\singlespacing
\clearpage
\section{Introduction}

An important aspect of hypothesis-driven research is \textit{preregistration}, an open science practice that consists of the specification of research question(s), method(s) and analysis plan(s) before data collection. Preregistration is a relatively simple yet powerful tool for improving transparency in bilingualism research, and we suggest that, in keeping with current best practices in the experimental sciences, bilingualism researchers include preregistration as an essential component of hypothesis-driven research, along with other open science practices such as releasing materials, data and code alongside publications \parencite{ChambersEtAl2014,NosekEtAl2018,NosekLakens2014,NosekEtAl2018letter,open2015estimating}.

\Copy{R2clarify}{\revised{There are several positions regarding the goals of preregistration. Many researchers view it as a tool specific to confirmatory research because it can help assess the falsifiability of an experimental study's predictions, control for false positive error probability in null hypothesis significance testing (NHST), and mitigate researcher biases (e.g., \cite{Lakens2019prereg,chambers2019seven,NosekEtAl2019tics}).
Under this view, preregistration helps implement the distinction between confirmatory analyses (used for hypothesis testing) and exploratory analyses (used for hypothesis generation) (e.g., \cite{deGroot2014, chambers2019seven,NosekEtAl2018,NosekEtAl2019tics,WagenmakersEtAl2012}). More recently, preregistration has also been considered for qualitative research with the aim to make documentation of research plans more transparent \parencite{HavenVanGrootel2019}.}}\label{pageR2clarify}
\Copy{R2clarify1.2}{\revised{Other research groups acknowledge the contribution of preregistration to scientific transparency, but call into question the validity of the distinction between confirmatory and exploratory research, and the usefulness of preregistration to help implement this distinction (e.g.,\ \cite{DevezerEtAl2020prereg, SzollosiEtAl2019prereg,SzollosiDonkin2019prereg}, cf.\ \cite{WagenmakersPreregBlog}). From this point of view, a shift to the development of more explicit theories would make preregistration unnecessary.

In this paper, we take the position that preregistration is crucial to separate confirmatory from exploratory analyses.}}\label{pageR2clarify1.2} \Copy{clarify2}{\revised{In our view, the preregistration of confirmatory hypotheses can counter questionable research practices and unconscious biases (\nameref{sec:box1}). Consequently, it can enhance research transparency in confirmatory bilingualism (L2) research.}}\label{pageclarify2}
Concerns about (non-)transparency and researcher biases are well-known in psychological science \parencite{Wicherts2006,simmons2011false}. \Copy{clarify3}{\revised{L2 research is similarly affected by a lack of clarity about pre-data collection hypotheses and analysis plan choices.}}\label{pageclarify3} This problem is compounded by the fact that L2 studies rarely release their research materials \parencite{Derrick2016,MarsdenThompsonPlonsky2018} or their data \parencite{Larson-HallPlonsky2015,,BolibaughEtAl2020}. 

To address these issues, two journals in the field of bilingualism, \textit{Language Learning} and \textit{Bilingualism: Language and Cognition}, have introduced a new type of article, Registered Reports, which allows researchers to submit their hypotheses, methods, and analysis protocols for peer review prior to data collection \parencite{MarsdenEtAl2018RegReports}.
Here, we discuss a different approach: \Copy{clarify4}{\revised{non-peer reviewed preregistration using open science platforms such as the Open Science Framework (OSF, \url{https://osf.io/}) or AsPredicted (\url{https://aspredicted.org/}).}}\label{pageclarify4}
On these platforms, researchers have the opportunity to create a public or private, time-stamped, non-modifiable record of a planned study
prior to data inspection, either before or during data collection.
Here, we argue that non-peer reviewed preregistration can counteract the questionable research practices presented below. We first illustrate them with an example from our own work on native (L1) sentence processing. Then, we discuss correlates in the L2 literature and explain how non-peer reviewed preregistrations can improve L2 research.


\begin{tcolorbox}[breakable, enhanced]

\subsection[Box 1]{Box 1: Three questionable research practices and biases}\label{sec:box1}

\begin{itemize}
\item \textbf{The garden of forking paths}\\
In hypothesis-driven research, there are many possible data analysis paths, and one of several potential paths can be selectively chosen and reported \parencite{gelman2013garden,GelmanLoken2014crisis}. For example, one could choose a particular measure, region of interest or time-window that was not originally selected for analysis, or delete outliers based on an arbitrary criterion. \Copy{clarify5}{\revised{Such multiple analysis paths cumulatively create so many researcher degrees of freedom that one can describe them using a decision tree.}}\label{pageclarify5} This bias is often an unconscious one (\cite{gelman2013garden}, pp.\ 9-10):
\begin{quote}
It’s not that the researchers performed hundreds of different comparisons and picked ones that were statistically significant. Rather, they start with a somewhat-formed idea in their mind of
what comparison to perform, and they refine that idea in light of the data.
(\dots) they are using their scientific common sense to formulate their hypotheses in a reasonable way, given the data they have. The mistake is in thinking that, if the particular path that was chosen yields statistical significance, that this is strong evidence in favor of the hypothesis.
\end{quote}
\item \textbf{Multiple testing}\\
For purely statistical reasons, if one conducts enough statistical tests, some test will eventually come out significant. For example, in psycholinguistic eye-tracking reading research, one can easily end up conducting dozens of statistical tests to evaluate a single hypothesis. Simulations in \textcite{MalsburgAngele2016} demonstrate that multiple analyses in eye-tracking dramatically inflate Type I error, leading to a large proportion of false positive rejections of the null hypothesis. 
\item \textbf{Post-hoc hypothesizing}\\
When data is analyzed without having explicitly stated the predictions, one may easily convince oneself that an unforeseen result was expected all along, and subsequently report this unexpected finding as a confirmatory one. This bias is commonly referred to as `hypothesizing after the results are known' (\textit{HARKing}) \parencite{simmons2011false,Kerr1998}. This can skew the scientific record with less well-grounded theories, cherry-picked \textit{after the fact} \parencite{chambers2019seven}.
\end{itemize}

\end{tcolorbox}


<<echo=FALSE>>=
critt<-qt((0.05/6)/2,df=39)
critz<-qnorm((0.05/6)/2)
@

<<dillonloaddata, include=TRUE, echo=FALSE, warning=TRUE, error=TRUE, message=TRUE>>=
# Bayesian analysis of original Experiment 1 from Dillon et al 2013
orig <- read.table("../data/originaldataExp1/data_experiment1_jml.txt",header=TRUE)
orig$subj <- factor(orig$subj)
orig$item <- factor(orig$item)
orig$cond <- factor(orig$cond)
# rename conditions to match our data
levels(orig$cond) <- c('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h')
# cast data to wide format to have the same data structure as in the analysis of our data above
orig <- dcast(orig, subj+item+cond+region ~ fixationtype, value.var = "value")

orig <- orig[, c('subj', 'item', 'cond', 'region', 'ff','fp', 'pr',  'rp', 'rr', 'tt')]
# rename columns:
colnames(orig) <- c('subj', 'item', 'cond', 'roi', 'FFD','FPRT', 'FPR', 'RPD','RRT','TFT')
@

<<dillonvars, include=TRUE, echo=FALSE, warning=TRUE, error=TRUE, message=TRUE>>=
Nsubj_orig <- length(unique(factor(orig$subj)))
Nitem_orig <- length(unique(factor(subset(orig, cond!='filler')$item)))
@

<<dilloncontrasts, include=TRUE,  echo=FALSE, warning=TRUE, error=TRUE, message=TRUE>>=
# Condition Labels (using our labels; same contrasts as above in analysis of our data); for documentation of contrasts see above. 

orig$Dep <- ifelse(orig$cond %in% c('a', 'b', 'c', 'd'), .5, -.5) # main effect of dependency type: agr=0.5, refl=-0.5
orig$Gram <- ifelse(orig$cond %in% c('a', 'b', 'e', 'f'), -.5, .5) # main effect of grammaticality: gram=-.5, ungram=.5
orig$Int_gram <- ifelse(orig$cond %in% c('a','e'), .5, ifelse(orig$cond %in% c('b', 'f'), -.5, 0) ) # interference in grammatical sentences: distr-match=0.5, distr-mismatch=-0.5
orig$Int_ungram <- ifelse(orig$cond %in% c('d', 'h'), .5, ifelse(orig$cond %in% c('c', 'g'), -.5, 0)) # interference in ungrammatical sentences: distr-match=0.5, distr-mismatch=-0.5
orig$DepxInt_gram <-ifelse(orig$cond %in% c('a', 'f'), .5, ifelse(orig$cond %in% c('b', 'e'), -.5, 0))
orig$DepxInt_ungram <- ifelse(orig$cond %in% c('d', 'g'), .5, ifelse(orig$cond %in% c('c', 'h'), -.5, 0))
orig$DepxGram <- ifelse(orig$cond %in% c('c', 'd', 'e', 'f'), .5, -.5)

#orig$DepxInt <- ifelse(orig$cond %in% c('a', 'd', 'f', 'g'), 0.5, -0.5)
#orig$Int <- ifelse(orig$cond %in% c('a', 'd', 'e', 'h'), 0.5, -0.5) # main effect of interference: int=0.5, no int=-0.5
#orig$GramxInt <- ifelse(orig$cond %in% c('b', 'd', 'f', 'h'),0.5,-0.5)
#orig$DepxGramxInt <- ifelse(orig$cond %in% c('b', 'd', 'e', 'g'), 0.5, -0.5)

orig$Int_gram_refl <- ifelse(orig$cond %in% c('e'), .5, ifelse(orig$cond %in% c('f'), -.5, 0))
orig$Int_gram_agr <- ifelse(orig$cond %in% c('a'), .5, ifelse(orig$cond %in% c('b'), -.5, 0))
orig$Int_ungram_refl <- ifelse(orig$cond %in% c('h'), .5, ifelse(orig$cond %in% c('g'), -.5, 0))
orig$Int_ungram_agr <- ifelse(orig$cond %in% c('d'), .5, ifelse(orig$cond %in% c('c'), -.5, 0))
@

<<dilloncrit, include=TRUE,echo=FALSE, warning=TRUE, error=TRUE, message=TRUE>>=
# in original data, critical region is 
crit_orig <- subset(orig, roi==5 & cond!='filler') 
@

<<analysesDillon,eval=FALSE,echo=FALSE,cache=TRUE>>=
library(lme4)
mFFD <- lmer(FFD~ 1+Dep+Gram+DepxGram+Int_gram+Int_ungram+DepxInt_gram+DepxInt_ungram+ (1+DepxInt_ungram ||subj)+(1+DepxInt_ungram ||item),crit_orig,control=lmerControl(calc.derivs=FALSE))
resFFD<-summary(mFFD)$coefficients[8,]

mFPRT<-lmer(FPRT~ 1+Dep+Gram+DepxGram+Int_gram+Int_ungram+DepxInt_gram+DepxInt_ungram+ (1+DepxInt_ungram ||subj)+(1+DepxInt_ungram ||item),crit_orig,control=lmerControl(calc.derivs=FALSE))
resFPRT<-summary(mFPRT)$coefficients[8,]

mFPR<-glmer(FPR~ 1+Dep+Gram+DepxGram+Int_gram+Int_ungram+DepxInt_gram+DepxInt_ungram+ (1+DepxInt_ungram ||subj)+(1+DepxInt_ungram ||item),crit_orig,family=binomial())
resFPR<-summary(mFPR)$coefficients[8,1:3]

mRPD<-lmer(RPD~ 1+Dep+Gram+DepxGram+Int_gram+Int_ungram+DepxInt_gram+DepxInt_ungram+ (1+DepxInt_ungram ||subj)+(1+DepxInt_ungram ||item),crit_orig,control=lmerControl(calc.derivs=FALSE))
resRPD<-summary(mRPD)$coefficients[8,]

mRRT<-lmer(RRT~ 1+Dep+Gram+DepxGram+Int_gram+Int_ungram+DepxInt_gram+DepxInt_ungram+ (1+DepxInt_ungram ||subj)+(1+DepxInt_ungram ||item),crit_orig,control=lmerControl(calc.derivs=FALSE))
resRRT<-summary(mRRT)$coefficients[8,]

mTFT<-lmer(TFT~ 1+Dep+Gram+DepxGram+Int_gram+Int_ungram+DepxInt_gram+DepxInt_ungram+ (1+DepxInt_ungram ||subj)+(1+DepxInt_ungram ||item),crit_orig,control=lmerControl(calc.derivs=FALSE))

mTFTmax<-lmer(log(TFT+1)~ 1+Dep+Gram+DepxGram+Int_gram+Int_ungram+DepxInt_gram+DepxInt_ungram+ (1+Dep+Gram+DepxGram+Int_gram+Int_ungram+DepxInt_gram+DepxInt_ungram ||subj)+(1+Dep+Gram+DepxGram+Int_gram+Int_ungram+DepxInt_gram+DepxInt_ungram ||item),crit_orig,control=lmerControl(calc.derivs=FALSE))
resTFT<-summary(mTFT)$coefficients[8,]
resTFTmax<-summary(mTFTmax)

res<-rbind(resFFD,resFPRT,resFPR,resRPD,resRRT,resTFT)
res<-data.frame(DepVar=c("FFD","FPRT","FPR","RPD","RRT","TFT"),res)
@

<<xtableres,echo=FALSE,eval=FALSE>>=
xtable(res,include.rownames=FALSE)
@



<<loaddata, include=TRUE, cache=TRUE, echo=FALSE, warning=TRUE, error=TRUE, message=TRUE>>=
d1 <- read.table("../data/dataDillonRepV1.txt",header=TRUE)
# delete subject 201t (same subject as 201t_2)
d1 <- subset(d1, d1$subject!='201t')
# rename subject 168bry and 201t_2
d1$subject <- ifelse(d1$subject=='168bry', '168b', as.character(d1$subject))
d1$subject <- ifelse(d1$subject=='201t_2', '201b', as.character(d1$subject))
d1$subject <- factor(d1$subject)
# edit subject ids such that they match with the working memory data
d1$subject <- unlist(strsplit(as.character(d1$subject), split=c('b')))
# remove 0s at id onset
d1$subject<-as.factor(as.integer(d1$subject))

d2 <- read.table("../data/dataDillonRepV2.txt",header=TRUE)
#summary(factor(d1$RESPONSE_ACCURACY)) 
#summary(factor(d2$RESPONSE_ACCURACY)) # 1=correct; 0=incorrect; -1=trial without a question
#xtabs(~RESPONSE_ACCURACY+item, d2)
# items followed by comprehension question
q <- sort(unique(subset(d2, RESPONSE_ACCURACY!=-1)$item))
# code absence of question as -1 in RESPONSE_ACCURACY of d1 (analoguously to coding in d2)
d1$RESPONSE_ACCURACY <- ifelse(!d1$item %in% q, -1, d1$RESPONSE_ACCURACY)

# Correct some wrong subject id labels in dataset V2 (=d2)
## 321_dr2 is the correct file for participant 321; 321_dr3 was incorrectly labeled and should have bene 386_dr3.
d2$subject <- factor(ifelse(d2$subject=='321_dr3', '386_dr3', as.character(d2$subject)))
##337_dr1 is the correct file for participant 337; 337_dr2 was incorrectly labeled and should be 338_dr1.
d2$subject <- factor(ifelse(d2$subject=='337_dr2', '338_dr1', as.character(d2$subject)))
# The data for participant 334 (from V2) is labeled as 223_dr6 => should be 334_dr6.
d2$subject <- factor(ifelse(d2$subject=='223_dr6', '334_dr6', as.character(d2$subject)))
# Data for participant 331 is labeled as 362_dr3 => should be 331_dr3.
d2$subject <- factor(ifelse(d2$subject=='2362_dr3', '331_dr3', as.character(d2$subject)))


d2$subject <- unlist(strsplit(as.character(d2$subject), split='[_dlr].*'))
d2$subject <- factor(d2$subject)


d <- rbind(d1,d2)
#colnames(d)
d$FPR <- as.numeric(as.logical(d$RBRC)) # first-pass regression
d$REG <- as.numeric(as.logical(d$TRC)) # whether or not there was an regression
d <- d[, c('subject', 'item', 'condition', 'RESPONSE_ACCURACY', 'roi', 'FFD','FPRT',  'FPR', 'RPD', 'RRT','TFT')]
colnames(d) <- c('subj', 'item', 'cond', 'acc', 'roi', 'FFD','FPRT',  'FPR', 'RPD', 'RRT','TFT')
d$item <- factor(d$item)
d$subj <- factor(d$subj)

#write.table(d, file='data/dataJMVV.txt', sep = '\t')
#d <- read.table(file='data/dataJMVV.txt', sep = '\t')
@

<<vars, include=TRUE, cache=TRUE, echo=FALSE, warning=TRUE, error=TRUE, message=TRUE>>=
Nsubj <- length(unique(factor(d$subj)))
#Nsubj_noWM <- length(which(is.na(op$span))) # number of subjects from which no wm data recorded
Nitem <- length(unique(factor(subset(d, cond!='filler')$item)))
Nfiller <- length(unique(factor(subset(d, cond=='filler')$item)))
@

<<accs, include=TRUE, cache=TRUE, echo=FALSE, warning=TRUE, error=TRUE, message=TRUE>>=
dq <- subset(d, item %in% q & roi==1) # comprehension accuracies
acc <- round(tapply(dq$acc, dq$cond, mean), 2)
# remove fillers
d <- subset(d, cond!='filler')
@

<<contrasts, include=TRUE, cache=TRUE,  echo=FALSE, warning=TRUE, error=TRUE, message=TRUE>>=
# Condition Labels (our labels are different from the ones used by Dillon et al 2013)
# a - d Agreement conditions.  
# a. grammatical, interference 
# b. grammatical, no interference
# c. ungrammatical, no interference
# d. ungrammatical, interference

# e - h Reflexives conditions. 
# e. grammatical, interference 
# f. grammatical, no interference 
# g. ungrammatical, no interference
# h. ungrammatical, interference


# Model 1: 
# main effect of dependency type (dep); positive effect => agr > refl
# main effect of grammaticality (gram);  positive effect => ungram > gram
# dep x gram
# interference within gram (int_gram); positive effect => inhib interference
# interference within ungram (int_ungram); => positive effect => inhib interference
# int_gram x dep; 
# int_ungram x dep; 
#### Only int_gram, int_ungram, int_gram x dep, int_ungram x dep are of theoretical interest


# Model 2: resolving interaction between dep and int_gram and between dep and int_ungram 
# main effect of dependency type (dep);  
# main effect of grammaticality (gram); 
# dep x gram
# interference within gram  in reflexives (int_gram_refl); positive effect => inhib interference
# interference within gram  in reflexives (int_gram_agr); positive effect => inhib interference
# interference within ungram in reflexives (int_ungram_refl); positive effect => inhib interference
# interference within ungram in agreement (int_ungram_agr); positive effect => inhib interference
#### Only int_gram_refl, int_gram_agr,  int_ungram_refl, int_ungram_agr are of theoretical interest

# Model 1wm: same as Model 1, with the following additional effects:
# main effect of working memory
# interactions between working memory and all other fixed effects (including interactions)

# Model 2wm: same as Model 2, with the following additional effects:
# main effect of working memory
# interactions between working memory and all other fixed effects (including interaction)


d$Dep <- ifelse(d$cond %in% c('a', 'b', 'c', 'd'), .5, -.5) # main effect of dependency type: agr=0.5, refl=-0.5
d$Gram <- ifelse(d$cond %in% c('a', 'b', 'e', 'f'), -.5, .5) # main effect of grammaticality: gram=-.5, ungram=.5
d$Int_gram <- ifelse(d$cond %in% c('a','e'), .5, ifelse(d$cond %in% c('b', 'f'), -.5, 0) ) # interference in grammatical sentences: distr-match=0.5, distr-mismatch=-0.5
d$Int_ungram <- ifelse(d$cond %in% c('d', 'h'), .5, ifelse(d$cond %in% c('c', 'g'), -.5, 0)) # interference in ungrammatical sentences: distr-match=0.5, distr-mismatch=-0.5
d$DepxInt_gram <-ifelse(d$cond %in% c('a', 'f'), .5, ifelse(d$cond %in% c('b', 'e'), -.5, 0))
d$DepxInt_ungram <- ifelse(d$cond %in% c('d', 'g'), .5, ifelse(d$cond %in% c('c', 'h'), -.5, 0))
d$DepxGram <- ifelse(d$cond %in% c('c', 'd', 'e', 'f'), .5, -.5)

d$DepxInt <- ifelse(d$cond %in% c('a', 'd', 'f', 'g'), 0.5, -0.5)
d$Int <- ifelse(d$cond %in% c('a', 'd', 'e', 'h'), 0.5, -0.5) # main effect of interference: int=0.5, no int=-0.5
d$GramxInt <- ifelse(d$cond %in% c('b', 'd', 'f', 'h'), 0.5, -0.5)
d$DepxGramxInt <- ifelse(d$cond %in% c('b', 'd', 'e', 'g'), 0.5, -0.5)

d$Int_gram_refl <- ifelse(d$cond %in% c('e'), .5, ifelse(d$cond %in% c('f'), -.5, 0))
d$Int_gram_agr <- ifelse(d$cond %in% c('a'), .5, ifelse(d$cond %in% c('b'), -.5, 0))
d$Int_ungram_refl <- ifelse(d$cond %in% c('h'), .5, ifelse(d$cond %in% c('g'), -.5, 0))
d$Int_ungram_agr <- ifelse(d$cond %in% c('d'), .5, ifelse(d$cond %in% c('c'), -.5, 0))
@

<<crit, include=TRUE, cache=TRUE, echo=FALSE, warning=TRUE, error=TRUE, message=TRUE>>=
crit <- subset(d, roi==12 & cond!='filler') 
@

<<analysesDillonrep,eval=FALSE,echo=FALSE,cache=TRUE>>=
mFFD <- lmer(FFD~ 1+Dep+Gram+DepxGram+Int_gram+Int_ungram+DepxInt_gram+DepxInt_ungram+ (1+DepxInt_ungram ||subj)+(1+DepxInt_ungram ||item),crit,control=lmerControl(calc.derivs=FALSE))
resFFD<-summary(mFFD)$coefficients[8,]

mFPRT<-lmer(FPRT~ 1+Dep+Gram+DepxGram+Int_gram+Int_ungram+DepxInt_gram+DepxInt_ungram+ (1+DepxInt_ungram ||subj)+(1+DepxInt_ungram ||item),crit,control=lmerControl(calc.derivs=FALSE))
resFPRT<-summary(mFPRT)$coefficients[8,]


mFPR<-glmer(FPR~ 1+Dep+Gram+DepxGram+Int_gram+Int_ungram+DepxInt_gram+DepxInt_ungram+ (1+DepxInt_ungram ||subj)+(1+DepxInt_ungram ||item),crit,family=binomial())
resFPR<-summary(mFPR)$coefficients[8,1:3]



mRPD<-lmer(RPD~ 1+Dep+Gram+DepxGram+Int_gram+Int_ungram+DepxInt_gram+DepxInt_ungram+ (1+DepxInt_ungram ||subj)+(1+DepxInt_ungram ||item),crit,control=lmerControl(calc.derivs=FALSE))
resRPD<-summary(mRPD)$coefficients[8,]

mRRT<-lmer(RRT~ 1+Dep+Gram+DepxGram+Int_gram+Int_ungram+DepxInt_gram+DepxInt_ungram+ (1+DepxInt_ungram ||subj)+(1+DepxInt_ungram ||item),crit,control=lmerControl(calc.derivs=FALSE))
resRRT<-summary(mRRT)$coefficients[8,]

mTFT<-lmer(TFT~ 1+Dep+Gram+DepxGram+Int_gram+Int_ungram+DepxInt_gram+DepxInt_ungram+ (1+DepxInt_ungram ||subj)+(1+DepxInt_ungram ||item),crit,control=lmerControl(calc.derivs=FALSE))

resTFT<-summary(mTFT)$coefficients[8,]

res<-rbind(resFFD,resFPRT,resFPR,resRPD,resRRT,resTFT)
res<-data.frame(DepVar=c("FFD","FPRT","FPR","RPD","RRT","TFT"),res)
@

<<xtableresrep,echo=FALSE,eval=FALSE>>=
xtable(res,include.rownames=FALSE)
@


\section{Possible pitfalls of hypothesis-driven research: An example from L1 sentence processing}\label{sec:L1example}

We briefly introduce our study, which attempted to replicate the findings of an eye-tracking reading study that compared the processing of two different syntactic dependencies \parencite{Dillon-EtAl-2013,JaegerMertzenVanDykeVasishth2019}. This example can be easily translated to bilingualism settings where, similar to our example, processing patterns are investigated for different syntactic constructions, but for different speaker groups, such as native vs.\ non-native speakers \parencite{FelserCunnings2012,GrueterEtAl2012}, or successive vs.\ simultaneous learners \parencite{LemmerthHopp2019,SabourinVinerte2015}. 

Our example concerns a phenomenon called \textit{agreement attraction}. For subject-verb agreement dependencies, previous work has shown that a processing disruption elicited by an ungrammatical plural verb can be weakened if a plural noun (an ``attractor’’) intervenes between the subject and the verb (as in \ref{ex:Dillon13}a vs.\ \ref{ex:Dillon13}b; \cite{wagersetal,pearlmutter1999agreement,Dillon-EtAl-2013}). 
Dillon and colleagues used a within-subjects design to examine whether the attraction effect extended to ungrammatical reflexive-antecedent dependencies, where an attractor matched the reflexive in number (\ref{ex:Dillon13}c vs.\ \ref{ex:Dillon13}d). 


\begin{exe}
\ex \label{ex:Dillon13}
\begin{xlist}
\item  \textit{Subject-verb agreement; attraction}\\
*The amateur bodybuilder who worked with the \textbf{personal trainers} amazingly \underline{were} competitive for the gold medal. 
\item \textit{Subject-verb agreement; no attraction}\\
*The amateur bodybuilder who worked with the \textbf{personal trainer} amazingly \underline{were} competitive for the gold medal.
\item \textit{Reflexive; attraction}\\
*The amateur bodybuilder who worked with \textbf{the personal trainers} amazingly injured \underline{themselves} on the lightest weights.
\item \textit{Reflexive; no attraction}\\
*The amateur bodybuilder who worked with \textbf{the personal trainer} amazingly injured \underline{themselves} on the lightest weights.
\end{xlist}
\end{exe}

Building on work by \textcite{sturt03}, they argued that, unlike subject-verb agreement configurations, the processing of antecedent-reflexive dependencies should be syntactically constrained \parencite{chomsky1981}. If so, attraction effects were expected in subject-verb dependencies but not in antecedent-reflexive dependencies, yielding an interaction between dependency type and attraction.

\textcite{Dillon-EtAl-2013} analyzed multiple reading measures and observed the predicted interaction only in total reading time.
This result was taken as support for the hypothesis that subject-verb agreement and reflexives show different susceptibility to agreement attraction and thus are differentially constrained by syntactic principles. 
In our large-sample replication study \parencite{JaegerMertzenVanDykeVasishth2019}, the goal was to replicate the statistically significant interaction in total reading time from the original study. 
Our confirmatory analysis of total reading time showed no effect, while the exploratory analyses of first-pass regressions and regression-path durations did (Table \ref{tab:dillonresults}).

\begin{table}[!htbp]
\centering
\setlength\tabcolsep{2.5pt}
\begin{tabular}{lrrr|rrr}
\hline
& \multicolumn{3}{l}{\textbf{Dillon et al.\ results}} & \multicolumn{3}{l}{\textbf{J\"ager et al.\ replication}}\\
& \multicolumn{3}{l}{\textbf{(N = 40)}} & \multicolumn{3}{l}{\textbf{(N = 181)}}\\
Dependent measure & Estimate & Std.\ Error & z/t value & Estimate & Std.\ Error & z/t value \\ 
  \hline
First fixation duration & 1.74 & 5.54 & 0.31     & -2.54 & 4.22 & -0.60 \\ 
First pass reading time & 0.46 & 16.16 & 0.03   & -3.14 & 7.74 & -0.41 \\ 
\textbf{First pass regressions} & -0.07 & 0.19 & -0.35  & \textbf{-0.20} & \textbf{0.09} & \textbf{-2.30} \\ 
\textbf{Regression-path duration} & -5.07 & 30.74 & -0.16 & \textbf{-44.02} & \textbf{21.09} & \textbf{-2.09} \\ 
Re-reading time & -64.86 & 39.90 & -1.63  & 5.28 & 13.11 & 0.40 \\
\textbf{Total reading time} & \textbf{-54.72} & \textbf{25.02} & \textbf{-2.19} & 2.19 & 14.24 & 0.15\\ 
   \hline
\end{tabular}
\caption{
Comparison of the findings by Dillon et al. (2013) and J\"ager et al. (2020). The table shows the interaction effect of Dependency type $\times$ Attraction, computed using generalized linear mixed models (effects on first-pass regressions were estimated using a logit link function). The interaction effect was expected to have a negative sign. Significant effects at a 0.05 $\alpha$-level are shown in bold.
Note that the published analyses in J\"{a}ger et al.\ (2020) differ from the ones we present here due to different model assumptions made in the  present paper for expository purposes.}\label{tab:dillonresults}
\end{table}


The study by Dillon and colleagues and our attempted replication serve to illustrate the potential issues of the garden of forking paths, multiple testing and post-hoc theorizing.\label{subsec:L1problems} 
First, even for a confirmatory replication study, where one analyzes the same region and reading measure that showed the interaction in the original study, garden of forking paths scenarios arise if an analysis path is not defined prior to data inspection. \Copy{clarify6}{\revised{For example, different decisions regarding statistical tests and outlier treatment could still be made after data inspection. 

Second, for the analyses of the Dillon et al.\ study and our replication study, six statistical tests were conducted. Testing six eye-tracking measures increases the Type I error probability from 5\% to 26.5\% (i.e., $1-0.95^6=0.265$).}}\label{pageclarify6}
It is possible to correct for multiple testing. For example, a Bonferroni correction would require an adjusted Type I error of $0.05/6$ for the six statistical tests we conducted, which implies that the absolute critical t-/z-value would be \Sexpr{round(abs(critz),2)}. If this criterion were used, there would be no significant effects in either the original study or the replication attempt (see observed z/t-values in Table \ref{tab:dillonresults}). \Copy{R3comment3}{\revised{A better solution to the multiple testing problem may be to avoid it altogether by having precise predictions about the dependent measure(s), and focus on (Bayesian) estimation of effects rather than NHST (e.g., \cite{Norouzian2020,gelmancarlin,Gelman14,kruschke2014doing}).}}\label{pageR3comment3}

Third, suppose that the effect that was expected a priori at the critical auxiliary verb or the reflexive had been found further downstream in the sentence or even before the critical region. Without specifying the critical region in advance, one could easily have found a post-hoc theory for the effect showing up in another region and reported this as if it had been predicted all along.

Finally, both the original and the replication study show some evidence of the effect of interest. However, the effect occurs in different measures across the two studies. Because of the exploratory nature of the first-pass regression and regression-path duration results in the replication attempt, we cannot treat these hypothesis tests as confirmatory ones. Exploratory analyses per se are an important part of doing science, but they should be presented as such (e.g., \cite{Bishop2020, deGroot2014, NosekEtAl2018}).



\section{Problematic research practices in L2 research}\label{sec:L2examples}

The issues above can also arise in L2 research. Two common examples of forks in the analysis path are outlier treatment and the selection of interest regions in reading studies. For example, a synthesis of methodological decisions in L2 self-paced reading (SPR) research showed a variety of outlier removal criteria across 64 studies, such as standard deviations around the mean, reading time cutoffs, or both (\cite{MarsdenThompsonPlonsky2018}; \Copy{R3comment2}{see \cite{NicklinPlonsky2020}, for discussion of outlier treatment).}\label{pageR3comment2}
Moreover, L2 reading studies on the same grammatical phenomena can vary substantially in their selection of interest regions. For a subset of the L2 SPR studies on local ambiguity processing synthesized in \textcite{MarsdenThompsonPlonsky2018}, some studies reported statistical analyses for the ambiguous sentence region, and other studies for some, or all, of the subsequent regions. In addition, the critical regions varied between studies, consisting of a single word or several words combined. 

A closely related problem to the selective reporting of interest regions is conducting statistical tests for many different regions, and/or eye-tracking measures. \textcite{Godfroid2020} reported that an average of 3.4 eye-tracking measures per study are analyzed in the L2 eye-tracking literature, further inflating Type I error probability. 
\Copy{R3comment3.2}{\revised{The Type I error issue might be particularly prevalent in L2 studies because many of them use frequentist NHST and only report binary decisions about the presence or absence of an effect without also reporting effect estimates \parencite{MarsdenThompsonPlonsky2018}. One unfortunate consequence is that other researchers cannot gain knowledge about the magnitude of an effect across studies, or conduct meta-analyses due to the lack of information from previous studies (\cite{Plonsky2013,Larson-HallPlonsky2015,PlonskyOswald2014,AlHoorieVitta2019}; for an introduction to meta-analyses in bilingualism research, see \cite{PlonskyOswald2015meta,PlonskyEtAl2020meta}).}}\label{pageR3comment3.2}

Finally, as in our example on L1 processing, post-hoc hypothesizing, i.e., changing a hypothesis to match the findings, may reduce the reproducibility of L2 research \parencite{MarsdenEtAl2018RegReports,MarsdenEtAl2018L2repl,chambers2019seven}. 
Possibly partly due to the issues raised above, and low statistical power \parencite{cohen1962statistical, powerbookcohen, Brysbaert2020}, inconsistent findings also occur in L2 research. Some examples include the role of crosslinguistic influence in syntactic processing \parencite{DussiasDietrichVillegas2015,LagoMoscaStutter2020}, the existence of a bilingual advantage in attentional systems \parencite{Bialystok2017,PaapEtAl2018}, and the role of morphological decomposition in inflected vs.\ derived forms during word recognition in native vs.\ non-native speakers \parencite{ClahsenVerissimo2016,FeldmanKroll2019}.
Next, we discuss how a non-peer reviewed preregistration can be implemented to improve L2 research.


\section{Non-peer reviewed preregistration in psycholinguistic research}\label{sec:prereg}

For preregistration to counter questionable research practices and biases, it is not sufficient to a priori specify the dependent measure(s), because many researcher degrees of freedom remain. A complete preregistration requires a full description of the research questions and hypotheses, study design, methods, speaker group selection criteria, data collection procedure, participant sample size or stopping rule, outcome variable(s), as well as an analysis plan including statistical models, information on data exclusion and statistical inference criteria.
This does not only ensure greater transparency, but it can also keep in check one's biases because analysis decisions are made public prior to data analysis, preventing selective reporting of effects. For example, assume that for a planned study we preregister no outlier exclusions, but later find an effect only when removing certain data points. This could be reported as an exploratory finding. 
Without preregistration, it may be tempting to report the most `interesting' result as confirmatory, preventing other researchers from evaluating the findings in light of the analysis choices. 
In addition, if our published preregistration committed to a predicted effect for a particular region and measure, based on theory or previous findings, we can no longer convince ourselves that a surprising result was originally predicted and restate the hypotheses post-hoc. 

One may argue that if one has strong theoretical predictions, preregistration is redundant because the analysis choices are predetermined by the theory. However, \textcite{SilberzahnEtAl2018} \Copy{R3comment1}{\revised{ convincingly illustrated that different analysis choices can be made even under highly constraining conditions. Their study recruited 29 research groups in the psychological sciences to answer \textit{the same research question for one particular dataset}. Of the 29 groups, 20 observed a significant and nine a non-significant result. Strikingly, the range of effect estimates reported by the different research groups allowed for different conclusions.}}\label{pageR3comment1}

Although we take the view that preregistration without peer review can be an effective way to reduce unconscious biases in one’s work, the lack of peer review means that the preregistration of a study can be as thorough or as vague as the researcher deems appropriate. Vaguely specified research plans still allow for many possible analysis paths, and selective reporting of effects. 
Consequently, it is up to the scientific community to make non-peer reviewed preregistration a success or a failure: only a thoroughly implemented preregistration and a precisely followed research plan can reduce unconscious biases and help to separate confirmatory hypothesis tests from exploratory ones.


\subsection{Selecting dependent measures for a preregistration}\label{sec:prereg2}

If one wants to preregister a study, but lacks prior knowledge of a particular phenomenon, an experiment could be piloted and exploratory analyses conducted to identify which measure(s) show the predicted effect. One could then generate hypotheses from this and test them in a confirmatory study (e.g., \cite{NicenboimEtAlCogSci2018, NicenboimPreactivation2019}). If, on the other hand, there \textit{are} previous findings on a phenomenon, these could serve as the basis for a preregistration. However, when the literature shows equivocal results as discussed above, what steps could be taken to consolidate the support in favor of or against a theory? This is not straightforward. For example, in the \textcite{Dillon-EtAl-2013} study and our replication study, the effect of interest was observed in different reading measures. If, based on linguistic theory, we believe that the effect of interest should be found in earlier reading measures (first-pass regression and regression-path duration as in our replication study), the only way to test this is by conducting a replication study. \Copy{R3comment3.3}{\revised{This replication should aim for a sufficiently large participant sample and a sufficiently precise effect estimate}}\label{pageR3comment3.3}, and specify the dependent measure(s) and critical region(s) in advance. Otherwise, in a future study we may find some other dependent measure showing the effect, which may again tempt us to draw a bullseye around the arrow that happened to land where it did.



\subsection{How to get started with a non-peer reviewed preregistration}\label{sec:prereg3}

Preregistration templates are available on OSF and AsPredicted for novel studies as well as for replication studies (e.g., \url{https://bit.ly/OSFtemplates}; \url{https://bit.ly/AsPredtemplate}).
If one prefers to create a Registered Report-type preregistration (i.e., in manuscript format), it is possible to upload a preregistration manuscript on OSF. It is not enough to upload this document to the project's public repository, because the preregistration could be removed or replaced at any point. Rather, one needs to create a time-stamped, non-editable version which can be made public either immediately or it can be embargoed until, for example, the associated paper is submitted or published. If the preregistration is withdrawn at any stage after creating a ``frozen'' version of it, some meta data (title, authors, description, reason for withdrawing preregistration) will remain publicly available.
A new version of the preregistration can be made available before the data are inspected. We have previously made attempts at such manuscript-style preregistration, e.g., for \textcite{VasishthMertzenJaegerGelman2018} and \textcite{MertzenEtAl2020} (see \url{https://osf.io/dgewb} and \url{https://osf.io/9qgrk}, respectively, for the non-editable preregistrations). 


\section{Conclusion}

We have used examples from L1 sentence processing and the L2 literature to illustrate some of the problems that can arise during the research process. We then discussed how preregistration allows researchers to better separate confirmatory and exploratory analyses, which can help them counter questionable research practices and unconscious biases. Our view is that, if done thoroughly, non-peer reviewed preregistration would greatly benefit the bilingualism community. We suggest that the hypothesis-driven L2 research process should standardly include preregistration, in addition to the release of materials, data and code upon publication to increase research transparency and reproducibility.



\section{Acknowledgements}

This research was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) – project number 317633480 – SFB 1287, Projects B03 and Q (PIs: Shravan Vasishth and Ralf Engbert).


\printbibliography

\clearpage

\doublespacing

\end{document}


