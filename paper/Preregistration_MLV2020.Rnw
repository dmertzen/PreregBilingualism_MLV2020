\documentclass[doc,floatsintext,12pt]{apa6}
%\documentclass[man]{apa6}

%\usepackage{hyperref}
\usepackage{graphicx}

\usepackage[american]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
%% customize as needed:
\usepackage[backend=biber,style=apa]{biblatex}
%\DeclareNameAlias{author}{last-first}

%\DeclareFieldFormat{labelnumberwidth}{\mkbibbrackets{#1}}

%\defbibenvironment{bibliography}
%  {\list
%     {\printtext[labelnumberwidth]{%
%      \printfield{labelprefix}%
%      \printfield{labelnumber}}}
%     {\setlength{\labelwidth}{\labelnumberwidth}%
%      \setlength{\leftmargin}{\labelwidth}%
%      \setlength{\labelsep}{\biblabelsep}%
%      \addtolength{\leftmargin}{\labelsep}%
%      \setlength{\itemsep}{\bibitemsep}%
%      \setlength{\parsep}{\bibparsep}}%
%      \renewcommand*{\makelabel}[1]{\hss##1}}
%  {\endlist}
%  {\item}

\usepackage{fancyvrb}

%\usepackage{newfloat}
%\DeclareFloatingEnvironment[
%    fileext=los,
%    listname=List of Schemes,
%    name=Fig.,
%    placement=!htbp,
%    within=section,
%]{fig}

\usepackage{url}
\usepackage[colorlinks=true,
            linkcolor=blue,
            urlcolor=blue,
            citecolor=black]{hyperref}

\usepackage{lscape} % landscape table
\usepackage{longtable}
\usepackage{threeparttablex}
\usepackage{booktabs}
\usepackage{multirow} % multirows in tables 
\usepackage{bigdelim} % curly braces in table
\usepackage{xcolor,colortbl}

\usepackage[export]{adjustbox}
\usepackage{caption, subcaption, floatrow}

\usepackage{mathtools}
\makeatletter
 
\newcommand{\explain}[2]{\underset{\mathclap{\overset{\uparrow}{#2}}}{#1}}
\newcommand{\explainup}[2]{\overset{\mathclap{\underset{\downarrow}{#2}}}{#1}}
 
\makeatother

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\newtheorem{definition}{Definition}[section]

\usepackage{microtype}
\usepackage[american]{babel}
\usepackage[utf8]{inputenc}
\usepackage{csquotes}
\usepackage{setspace}
\usepackage{geometry}
\usepackage{lineno,clipboard}
\newclipboard{reviews}
\openclipboard{reviews}

\usepackage[most]{tcolorbox}

%% for revisions
\newcommand{\revised}[1]{{\color{black}{#1}}}
%\newcommand{\revised}[1]{{\color{red}{#1}}}


%\usepackage{tikz}
\usepackage{todonotes}

\usepackage{gb4e}

%% act-r papers only
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\newcommand{\R}{\textsf{R}}
\newcommand{\actrcue}[1]{\texttt{\uppercase{#1}}}
% \newcommand{\match}[1]{$+$\texttt{#1}}
\newcommand{\match}[1]{\texttt{+\uppercase{#1}}}
% \newcommand{\mismatch}[1]{$-$\texttt{\uppercase{#1}}}
\newcommand{\mismatch}[1]{\texttt{-\uppercase{#1}}}
\newcommand{\featureset}[2]{$\{^{\texttt{\uppercase{#1}}}_{\texttt{\uppercase{#2}}}\}$}
\newcommand{\featuresetNP}[2]{$^{\texttt{\uppercase{#1}}}_{\texttt{\uppercase{#2}}}$}
\mathchardef\mhyphen="2D % Define a "math hyphen"
\newcommand\ccom{\mathop{c\mhyphen com}}
% \newcommand{\ignore}[1]{}
\newcommand{\me}{\mathrm{e}}
\newcommand{\TODO}[1]{{\color{red}{#1}}}
%\newcommand{\revised}[1]{#1}
\newcommand{\revisedII}[1]{#1}
\newcommand{\revisedIII}[1]{{\color{red}{#1}}}
\newcommand{\revFE}[1]{{\color{red}{#1}}}
\newcommand{\revSV}[1]{{\color{red}{#1}}}
\newcommand{\revIV}[1]{{\color{red}{#1}}}
\newcommand{\revV}[1]{{\color{blue}{#1}}}
%\newcommand{\exitem}{\refsteptucounter{example}\item[(\arabic{example})]}
%\newcounter{example}

\DeclareLanguageMapping{american}{american-apa}
\addbibresource{bibcleaned.bib}

\AtEveryBibitem{\clearfield{issue}}


\leftheader{Mertzen,Lago,Vasishth}
\title{The benefits of preregistration for hypothesis-driven bilingualism research}
\shorttitle{Preregistration in bilingualism language research}

\threeauthors{Daniela Mertzen}{Sol Lago}{Shravan Vasishth}

\threeaffiliations{Department of Linguistics, University of Potsdam, Potsdam, Germany}{Institute for Romance Languages and Literatures, Goethe University Frankfurt, Frankfurt, Germany}{Department of Linguistics, University of Potsdam, Potsdam, Germany}

\authornote{Correspondence: mertzen@uni-potsdam.de. All data and code are available from \url{https://osf.io/5ab7d/}.}
\note{\today}

\journal{Submitted to Bilingualism: Language and Cognition} 
\volume{} 

\keywords{Confirmatory analysis; preregistration; open science; psycholinguistics; bilingualism}


\abstract{Preregistration is an open science practice that focuses on statistically testing hypotheses specified before the data are inspected (hypothesis-driven, confirmatory research). Here, we discuss the benefits of preregistration for bilingualism research. Using a psycholinguistic example, we illustrate how preregistration serves to implement a clean distinction between hypothesis testing and data exploration. This distinction consequently helps the researcher to avoid casting post-hoc hypotheses and analyses as confirmatory ones. In keeping with the current best practices in the experimental sciences, preregistration, along with sharing data and code, should be an integral part of hypothesis-driven bilingualism research.}

\begin{document}


\maketitle

<<setup,include=FALSE,cache=FALSE,echo=FALSE>>=
library(MASS)
library(knitr)
library(xtable)
#library(papaja)
library(tidyverse)
library(rstan)
options(mc.cores = parallel::detectCores())
library(brms)
library(gridExtra)
library(bayesplot)
library(ggridges)
library(lme4)
library(reshape2)

#theme_set(theme_apa())

# set global chunk options, put figures into folder
options(warn=-1, replace.assign=TRUE)
opts_chunk$set(fig.path='figures/figure-', fig.align='center', fig.show='hold')
options(replace.assign=TRUE,width=75)
opts_chunk$set(dev='postscript')
opts_chunk$set(echo = TRUE)
@


%\doublespacing
\clearpage
\section{Introduction}

An important aspect of hypothesis-driven confirmatory research is \textit{preregistration}, an open science practice that consists of the specification of research question(s), method(s) and analysis plan(s) before data collection begins. Preregistration is a relatively simple yet powerful tool for improving transparency in bilingualism research, and we suggest that, in keeping with the current best practices in the experimental sciences, researchers standardly include preregistration as an essential component of the hypothesis-driven research process.
The motivation for preregistration is that it can help researchers to eliminate unconscious biases and to avoid questionable research practices such as selectively choosing and reporting one of many potential data analysis paths (\textit{`garden-of-forking-paths'}) as well as \textit{`HARKing'}---hypothesizing after the results are known \parencite{Kerr1998,simmons2011false}.
Preregistering a study helps to clearly separate confirmatory analyses (used for hypothesis testing) from exploratory analyses (used for hypothesis generation) \parencite{deGroot2014, NosekEtAl2018}. This distinction between confirmatory and exploratory research is often not strictly applied in bilingualism research. In this article, we point out that this distinction is crucial for robust science, and we discuss the benefits of preregistering a planned study. 


While concerns about (non-)transparency and post-hoc theorizing are well-known in psychological science \parencite{Wicherts2006,simmons2011false}, bilingualism research is similarly weighed down by the opacity of its pre-data collection hypotheses and analysis plan choices. This problem is compounded by the fact that L2 studies rarely release their research materials \parencite{Derrick2016,MarsdenThompsonPlonsky2018} or their data \parencite{Larson-HallPlonsky2015}. To address these issues, two journals in the field of bilingualism, \textit{Language Learning} and \textit{Bilingualism: Language and Cognition}, have recently introduced a new type of article, Registered Reports, which allows researchers to submit their hypotheses, methods, and analysis protocols for peer review prior to data collection \parencite{MarsdenEtAl2018RegReports}.
Here, our focus is on preregistration using open science platforms such as the Open Science Framework (OSF, \url{https://osf.io/}), or their dedicated registration platform Center for Open Science (\url{https://cos.io/prereg/}), and AsPredicted (\url{https://aspredicted.org/}). 
On these platforms, researchers have the opportunity to create a public or private, but time-stamped, non-modifiable record of a planned study \textit{before} the data is collected, or during the process of data collection, but before it is analyzed. This type of preregistration is not subject to the often time-consuming process of peer review.

\section{Why preregister? An example from sentence comprehension research}

To illustrate the benefits of preregistration, we use an example from our own work, which attempted to replicate the findings of a previous eye-tracking reading study investigating real-time sentence comprehension \parencite{Dillon-EtAl-2013,JaegerMertzenVanDykeVasishth2019}.

The two studies that we use as an example examined a grammaticality illusion phenomenon in two different syntactic configurations. The example can be easily translated to bilingualism settings where, similar to our example, differential processing patterns are investigated for different syntactic constructions in L2 learners \parencite{AlemanEtAl2018,TokowiczMacWhinney2005,FoucartFrenck-Mestre2012}. In bilingualism, the comparison of interest may also concern different speaker groups, such as native vs.\ non-native speakers \parencite{FelserCunnings2012,GrueterEtAl2012}, or successive vs.\ simultaneous learners \parencite{LemmerthHopp2019,SabourinVinerte2015}. 

Our grammaticality illusion example investigates a phenomenon called \textit{agreement attraction}. The subject-verb agreement dependencies shown in \ref{ex:Dillon13ungagr}a and \ref{ex:Dillon13ungagr}b are ungrammatical because the singular subject \textit{The amateur bodybuilder} does not have the same number marking as the plural auxiliary \textit{were}.
Interestingly, when a plural noun (a so-called ``attractor'', such as \textit{personal trainers})  intervenes between the subject and the auxiliary verb, a speedup in reading times is observed in \ref{ex:Dillon13ungagr}a, compared to \ref{ex:Dillon13ungagr}b where no plural noun is present \parencite{wagersetal,pearlmutter1999agreement}. This result suggests that the formation of subject-verb agreement dependencies is not always constrained by grammar.


\begin{exe}
\ex \label{ex:Dillon13ungagr}
\begin{xlist}
\item  \textit{Subject-verb agreement; attraction}\\
*The amateur bodybuilder who worked with the \textbf{personal trainers} amazingly \underline{were} competitive for the gold medal. 
\item \textit{Subject-verb agreement; no attraction}\\
*The amateur bodybuilder who worked with the \textbf{personal trainer} amazingly \underline{were} competitive for the gold medal.
\end{xlist}
\end{exe}

Dillon and colleagues used a within-subjects design to examine whether similar attraction effects were observed for reflexive-antecendent dependencies (as in \ref{ex:Dillon13ungrefl}). Building on work by \textcite{sturt03}, they argued that, unlike subject-verb agreement configurations, the processing of reflexive-antecedent dependencies should be syntactically constrained by Principle A of the binding theory \parencite{chomsky1981}, which rules out the plural attractor noun as an antecedent for the reflexive. Therefore, no difference in processing time is expected at the reflexive in condition \ref{ex:Dillon13ungrefl}a compared to \ref{ex:Dillon13ungrefl}b, i.e., no agreement attraction should be observed. Due to the predicted speedup in \ref{ex:Dillon13ungagr}a vs.\ \ref{ex:Dillon13ungagr}b (subject-verb agreement conditions) but not in \ref{ex:Dillon13ungrefl}a vs. \ref{ex:Dillon13ungrefl}b (reflexive conditions), an interaction between dependency type and attraction effect was expected.  

\begin{exe}
\ex \label{ex:Dillon13ungrefl}
\begin{xlist}
\item \textit{Reflexive; attraction}\\
*The amateur bodybuilder who worked with \textbf{the personal trainers} amazingly injured \underline{themselves} on the lightest weights.
\item \textit{Reflexive; no attraction}\\
*The amateur bodybuilder who worked with \textbf{the personal trainer} amazingly injured \underline{themselves} on the lightest weights.
\end{xlist}
\end{exe}

In an eyetracking study, it is possible to test for the interaction in a range of dependent measures, some of which are usually assumed to reflect `early’ and some `late’ cognitive processes \parencite{cliftonjr:emr,WileyVME2012}. \textcite{Dillon-EtAl-2013} conducted statistical tests for multiple dependent measures and observed evidence for the predicted interaction only in total reading time (Table \ref{tab:dillonresults}). 
This significant interaction seems to confirm the hypothesis that subject-verb agreement and reflexives show different patterns for the number agreement manipulation. 



\begin{table}[!htbp]
\centering
\setlength\tabcolsep{2.5pt}
\begin{tabular}{lrrr|rrr}
\hline
& \multicolumn{3}{l}{\textbf{Dillon et al.\ results}} & \multicolumn{3}{l}{\textbf{J\"ager et al.\ replication}}\\
& \multicolumn{3}{l}{\textbf{(N = 40)}} & \multicolumn{3}{l}{\textbf{(N = 181)}}\\
Dependent measure & Estimate & Std.\ Error & z/t value & Estimate & Std.\ Error & z/t value \\ 
  \hline
First fixation duration & 1.74 & 5.54 & 0.31     & -2.54 & 4.22 & -0.60 \\ 
First pass reading time & 0.46 & 16.16 & 0.03   & -3.14 & 7.74 & -0.41 \\ 
First pass regressions & -0.07 & 0.19 & -0.35  & \textbf{-0.20} & \textbf{0.09} & \textbf{-2.30} \\ 
Regression-path duration & -5.07 & 30.74 & -0.16 & \textbf{-44.02} & \textbf{21.09} & \textbf{-2.09} \\ 
Re-reading time & -64.86 & 39.90 & -1.63  & 5.28 & 13.11 & 0.40 \\
Total reading time & \textbf{-54.72} & \textbf{25.02} & \textbf{-2.19} & 2.19 & 14.24 & 0.15\\ 
   \hline
\end{tabular}
\caption{
Comparison of the findings by Dillon et al. (2013) and J\"ager et al. (2020). The table shows a summary of the difference in the attraction effect in the subject-verb agreement vs.\ reflexive conditions, computed using generalized linear mixed models (effects on first-pass regressions were estimated using a logit link function).
Note that the published analyses in J\"{a}ger et al.\ (2020) differ from the ones we present here due to different model assumptions made in the  present paper for expository purposes.
An interaction effect with a negative sign was expected. The so-called \textit{early} reading measures examined are first-fixation duration, first-pass reading time, first-pass regressions, and regression-path duration; \textit{late} measures are re-reading time, and total reading time. The rows with significant effects at a 0.05 $\alpha$-level are shown in bold.}\label{tab:dillonresults}
\end{table}

In our large-sample replication study \parencite{JaegerMertzenVanDykeVasishth2019}, the goal was to replicate the significant interaction found in total reading time in the original Dillon et al.\ study. Here, by `replicating' we mean reproducing the claimed statistically significant interaction with a negative sign in total reading time. Our confirmatory analysis of total reading time showed no effect, while the exploratory analyses of first-pass regressions and regression-path durations did (Table \ref{tab:dillonresults}).

The original study and the replication study both seem to show \textit{some} evidence of an interaction, i.e., an attraction effect for subject-verb agreement but not for reflexives; however, this interaction occurs in different measures across the two studies. Importantly, because of the exploratory nature of the first-pass regression and regression-path duration results in the replication attempt, we cannot treat these hypothesis tests as confirmatory ones \parencite{deGroot2014}. Therefore, the evidence for different processing profiles in agreement vs.\ reflexives is inconclusive at best. When we have equivocal results like these, what steps could be taken to come closer to answering our research question?  
We could conduct a confirmatory study. Here, all the usual researcher degrees of freedom (dependent measures, critical regions, etc.) should be fixed prior to collecting the data \parencite{chambers2019seven}; subsequently, hypothesis testing can be carried out. It is for such confirmatory hypothesis tests that preregistration is designed. Next, we discuss why confirmatory analyses and an exploratory analyses should be treated differently.


\subsection{Why exploratory analysis is not the same as confirmatory hypothesis testing}

\subsubsection{The garden-of-forking-paths problem}

Why is it problematic if we conduct an exploratory analysis, and then conclude that the predicted interaction has been found, if we obtain a statistically significant effect? 
The first important issue is the so-called \textit{garden-of-forking-paths} problem \parencite{gelman2013garden,GelmanLoken2014crisis}. There are many analysis paths that could have been chosen: one could have deleted some participants' data using some criterion (e.g., deleting extreme values that lie 2.5 or 3 standard deviations away from the mean), or chosen a different region of interest than originally planned. Such multiple analysis paths cumulatively create so many researcher degrees of freedom that one can describe them using a binary decision tree, hence the term garden-of-forking-paths.
We follow \textcite{gelman2013garden}'s terminology here to emphasize the point that the garden-of-forking-paths issue is often an unconscious bias (pp.\ 9-10):

\begin{quote}
It’s not that the researchers performed hundreds of different comparisons and picked ones that were statistically significant. Rather, they start with a somewhat-formed idea in their mind of
what comparison to perform, and they refine that idea in light of the data.
(\dots) they are using their scientific common sense to formulate their hypotheses in reasonable way, given the data they have. The mistake is in thinking that, if the particular path that was chosen yields statistical significance, that this is strong evidence in favor of the hypothesis.
\end{quote}
If we decide to report one of the many possible analysis paths, at least one of these paths will often yield a desired pattern  that looks like a signal but is really just noise.
<<echo=FALSE>>=
critt<-qt((0.05/6)/2,df=39)
critz<-qnorm((0.05/6)/2)
@
\subsubsection{Multiple testing}
The second important issue, which is closely related to the garden-of-forking-paths, is the multiple comparisons problem: for purely statistical reasons, if one conducts enough statistical tests, \textit{some} test will eventually come out significant. In psycholinguistics, one can easily end up conducting dozens or sometimes even hundreds of statistical tests to evaluate a single hypothesis. Simulations in \textcite{MalsburgAngele2016} demonstrated that multiple analyses in eyetracking dramatically inflate Type I error, leading to a large proportion of false positive rejections of the null hypothesis. 
For the analyses of the Dillon et al.\ study and the J\"{a}ger et al.\ replication study shown in Table \ref{tab:dillonresults}, six statistical tests were conducted. Testing six eye-tracking measures increases the false positive probability from 5\% to 26.5\% (i.e., 1$-$0.95^6=0.265).
It is possible to correct for multiple testing; for example, a Bonferroni correction would require an adjusted Type I error of $0.05/6$ for the six statistical tests we conducted, which implies that the absolute critical t-/z-value would be \Sexpr{round(abs(critz),2)}. Any observed t-/z-values would have to reach this threshold to be considered significant. None of the observed t-values in Table \ref{tab:dillonresults} for either the Dillon et al.\ (2013)  or the J\"{a}ger et al.\ (2020) study reach the \Sexpr{round(abs(critz),2)}-threshold. Thus, if we were to use the corrected Type I error, there would be no significant effects in either the original study or the replication attempt.
A better solution to the multiple testing problem than the Bonferroni correction would be to avoid multiple testing altogether and to have a precise prediction about the dependent measure(s) and the critical sentence region to be analyzed. 

\subsubsection{Post-hoc hypothesizing}
To conduct a confirmatory analysis of a precise prediction can not only counteract researcher degrees of freedom, but also hypothesizing after the results are known (\textit{HARKing}) \parencite{chambers2019seven, simmons2011false}. As an example, suppose that the effect that was expected at the critical auxiliary verb or the reflexive for the \textcite{Dillon-EtAl-2013} and \textcite{JaegerMertzenVanDykeVasishth2019} studies had been found further downstream in the sentence or even before the critical region. Without specifying the critical region a priori, one could easily have found an explanation for the effect showing up in another region and reported this as if it had been predicted all along.


<<dillonloaddata, include=TRUE, echo=FALSE, warning=TRUE, error=TRUE, message=TRUE>>=
# Bayesian analysis of original Experiment 1 from Dillon et al 2013
orig <- read.table("../data/originaldataExp1/data_experiment1_jml.txt",header=TRUE)
orig$subj <- factor(orig$subj)
orig$item <- factor(orig$item)
orig$cond <- factor(orig$cond)
# rename conditions to match our data
levels(orig$cond) <- c('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h')
# cast data to wide format to have the same data structure as in the analysis of our data above
orig <- dcast(orig, subj+item+cond+region ~ fixationtype, value.var = "value")

orig <- orig[, c('subj', 'item', 'cond', 'region', 'ff','fp', 'pr',  'rp', 'rr', 'tt')]
# rename columns:
colnames(orig) <- c('subj', 'item', 'cond', 'roi', 'FFD','FPRT', 'FPR', 'RPD','RRT','TFT')
@

<<dillonvars, include=TRUE, echo=FALSE, warning=TRUE, error=TRUE, message=TRUE>>=
Nsubj_orig <- length(unique(factor(orig$subj)))
Nitem_orig <- length(unique(factor(subset(orig, cond!='filler')$item)))
@

<<dilloncontrasts, include=TRUE,  echo=FALSE, warning=TRUE, error=TRUE, message=TRUE>>=
# Condition Labels (using our labels; same contrasts as above in analysis of our data); for documentation of contrasts see above. 

orig$Dep <- ifelse(orig$cond %in% c('a', 'b', 'c', 'd'), .5, -.5) # main effect of dependency type: agr=0.5, refl=-0.5
orig$Gram <- ifelse(orig$cond %in% c('a', 'b', 'e', 'f'), -.5, .5) # main effect of grammaticality: gram=-.5, ungram=.5
orig$Int_gram <- ifelse(orig$cond %in% c('a','e'), .5, ifelse(orig$cond %in% c('b', 'f'), -.5, 0) ) # interference in grammatical sentences: distr-match=0.5, distr-mismatch=-0.5
orig$Int_ungram <- ifelse(orig$cond %in% c('d', 'h'), .5, ifelse(orig$cond %in% c('c', 'g'), -.5, 0)) # interference in ungrammatical sentences: distr-match=0.5, distr-mismatch=-0.5
orig$DepxInt_gram <-ifelse(orig$cond %in% c('a', 'f'), .5, ifelse(orig$cond %in% c('b', 'e'), -.5, 0))
orig$DepxInt_ungram <- ifelse(orig$cond %in% c('d', 'g'), .5, ifelse(orig$cond %in% c('c', 'h'), -.5, 0))
orig$DepxGram <- ifelse(orig$cond %in% c('c', 'd', 'e', 'f'), .5, -.5)

#orig$DepxInt <- ifelse(orig$cond %in% c('a', 'd', 'f', 'g'), 0.5, -0.5)
#orig$Int <- ifelse(orig$cond %in% c('a', 'd', 'e', 'h'), 0.5, -0.5) # main effect of interference: int=0.5, no int=-0.5
#orig$GramxInt <- ifelse(orig$cond %in% c('b', 'd', 'f', 'h'),0.5,-0.5)
#orig$DepxGramxInt <- ifelse(orig$cond %in% c('b', 'd', 'e', 'g'), 0.5, -0.5)

orig$Int_gram_refl <- ifelse(orig$cond %in% c('e'), .5, ifelse(orig$cond %in% c('f'), -.5, 0))
orig$Int_gram_agr <- ifelse(orig$cond %in% c('a'), .5, ifelse(orig$cond %in% c('b'), -.5, 0))
orig$Int_ungram_refl <- ifelse(orig$cond %in% c('h'), .5, ifelse(orig$cond %in% c('g'), -.5, 0))
orig$Int_ungram_agr <- ifelse(orig$cond %in% c('d'), .5, ifelse(orig$cond %in% c('c'), -.5, 0))
@

<<dilloncrit, include=TRUE,echo=FALSE, warning=TRUE, error=TRUE, message=TRUE>>=
# in original data, critical region is 
crit_orig <- subset(orig, roi==5 & cond!='filler') 
@

<<analysesDillon,eval=FALSE,echo=FALSE,cache=TRUE>>=
library(lme4)
mFFD <- lmer(FFD~ 1+Dep+Gram+DepxGram+Int_gram+Int_ungram+DepxInt_gram+DepxInt_ungram+ (1+DepxInt_ungram ||subj)+(1+DepxInt_ungram ||item),crit_orig,control=lmerControl(calc.derivs=FALSE))
resFFD<-summary(mFFD)$coefficients[8,]

mFPRT<-lmer(FPRT~ 1+Dep+Gram+DepxGram+Int_gram+Int_ungram+DepxInt_gram+DepxInt_ungram+ (1+DepxInt_ungram ||subj)+(1+DepxInt_ungram ||item),crit_orig,control=lmerControl(calc.derivs=FALSE))
resFPRT<-summary(mFPRT)$coefficients[8,]

mFPR<-glmer(FPR~ 1+Dep+Gram+DepxGram+Int_gram+Int_ungram+DepxInt_gram+DepxInt_ungram+ (1+DepxInt_ungram ||subj)+(1+DepxInt_ungram ||item),crit_orig,family=binomial())
resFPR<-summary(mFPR)$coefficients[8,1:3]

mRPD<-lmer(RPD~ 1+Dep+Gram+DepxGram+Int_gram+Int_ungram+DepxInt_gram+DepxInt_ungram+ (1+DepxInt_ungram ||subj)+(1+DepxInt_ungram ||item),crit_orig,control=lmerControl(calc.derivs=FALSE))
resRPD<-summary(mRPD)$coefficients[8,]

mRRT<-lmer(RRT~ 1+Dep+Gram+DepxGram+Int_gram+Int_ungram+DepxInt_gram+DepxInt_ungram+ (1+DepxInt_ungram ||subj)+(1+DepxInt_ungram ||item),crit_orig,control=lmerControl(calc.derivs=FALSE))
resRRT<-summary(mRRT)$coefficients[8,]

mTFT<-lmer(TFT~ 1+Dep+Gram+DepxGram+Int_gram+Int_ungram+DepxInt_gram+DepxInt_ungram+ (1+DepxInt_ungram ||subj)+(1+DepxInt_ungram ||item),crit_orig,control=lmerControl(calc.derivs=FALSE))

mTFTmax<-lmer(log(TFT+1)~ 1+Dep+Gram+DepxGram+Int_gram+Int_ungram+DepxInt_gram+DepxInt_ungram+ (1+Dep+Gram+DepxGram+Int_gram+Int_ungram+DepxInt_gram+DepxInt_ungram ||subj)+(1+Dep+Gram+DepxGram+Int_gram+Int_ungram+DepxInt_gram+DepxInt_ungram ||item),crit_orig,control=lmerControl(calc.derivs=FALSE))
resTFT<-summary(mTFT)$coefficients[8,]
resTFTmax<-summary(mTFTmax)

res<-rbind(resFFD,resFPRT,resFPR,resRPD,resRRT,resTFT)
res<-data.frame(DepVar=c("FFD","FPRT","FPR","RPD","RRT","TFT"),res)
@

<<xtableres,echo=FALSE,eval=FALSE>>=
xtable(res,include.rownames=FALSE)
@

\subsection{How to clearly separate confirmatory and exploratory research}
\subsubsection{Preregistration of confirmatory hypotheses after a pilot experiment}
If we lack prior knowledge of a particular psycholinguistic phenomenon, we could pilot an experiment, conduct as many exploratory analyses as desired to identify which reading measure(s) might show the predicted effect, and generate hypotheses from this. 

This is where preregistration comes in. After the exploration stage, we preregister our prediction of the expected effect \textit{for a particular reading measure at a particular region}. 
We then re-run our study with the same items and a new (larger) participant sample, conducting the statistical analysis predefined in our time-stamped preregistration document. This will be our confirmatory analysis used for statistical inference. Of course, we are also free to conduct exploratory analyses for the new data set, but this exploration should be clearly identified as such. Recent implementations of this distinction between exploratory vs.\ confirmatory stage in psycholinguistics are \textcite{NicenboimEtAlCogSci2018, NicenboimPreactivation2019}. We want to reiterate that exploratory analyses per se are an important part of doing science. In fact, \textcite{Bem2004}, for example, actively encourages us to go on so-called ``fishing expeditions'', but such statements can easily be misunderstood. A recent paper by \textcite{Bishop2020} agrees with \textcite{Bem2004} but with the following caveat:
\begin{quote}
The problem comes when exploratory research is repackaged as if it were hypothesis testing, with the hypothesis invented after observing the data (HARKing), and the paper embellished with p-values that are bound to be misleading because they were p-hacked
from numerous possible values, rather than derived from testing an a priori hypothesis.
\end{quote}

\subsubsection{Preregistration of confirmatory hypotheses based on previous research}
If we already know something about the effect of interest from the previous literature, the dependent measure and analysis region previously reported to be significant could serve as the basis for a preregistration of a confirmatory replication study. This is not always straightfoward. For example, \textcite{Dillon-EtAl-2013} tested for an effect in multiple reading measures, and found the predicted interaction in total reading times, while our replication study only observed this interaction during the exploratory analysis stage in first-pass regressions and regression-path durations. Now, let's assume that, based on linguistic theory, we believe that the interaction should be found in the early stages of reading. If we want to get closer to answering the question of whether the first-pass regressions and regression-path duration results truly reflect evidence of agreement attraction in subject-verb dependencies but not in reflexives, the only way to test this is by conducting a planned study with a sufficiently large participant sample size where the analysis plan is laid out in advance. Otherwise, in a future study we may find some \textit{other} dependent measure showing the effect, which may again tempt us to draw a bullseye around the arrow that happened to land where it did.


In a preregistration, we can specify the expected interaction at the critical sentence region, in \textit{first-pass regressions} based on our previous outcome and based on linguistic theory. If planning to carry out such a confirmatory test, it is not sufficient to specify the dependent measure and the to-be-tested region. Too many researcher degrees of freedom remain. A complete preregistration requires a full description of the research questions and hypotheses, study design, methods, data collection procedure, participant sample size, outcome variable(s), an analysis plan including statistical models and information on data exclusion and statistical inference criteria. Preregistration templates are available on OSF and AsPredicted for novel as well as for replication studies.
If one prefers to create a Registered Report-type preregistration (i.e., in manuscript format), it is possible to upload a preregistration manuscript (pre-data collection or, at least, pre-data analysis). Importantly, it is not enough to upload this document to the project's public repository, because the preregistration could be removed or replaced at any point, but rather, one needs to create a time-stamped version. This time-stamped, non-editable version can be made public either immediately or it can be embargoed until, for example, the associated paper is submitted or published. If the registration is withdrawn at any stage after creating a ``frozen'' version of it, some meta data (the title, the authors, a short description, reason for withdrawing the preregistration) will remain publicly available (see \url{http://bit.ly/withdrawprereg}). A new version of the preregistration can be made available at any point before data is analyzed.
If opting for a manuscript-style registration, we have found it helpful to use the existing templates as a checklist to ensure that no important questions have been left unanswered in the manuscript. 

We have previously made an attempt at such a manuscript-style preregistration (\textcite{VasishthMertzenJaegerGelman2018}: \url{https://osf.io/eyphj/}). The frozen, non-editable version can be inspected at \url{https://osf.io/dgewb}. 



<<loaddata, include=TRUE, cache=TRUE, echo=FALSE, warning=TRUE, error=TRUE, message=TRUE>>=
d1 <- read.table("../data/dataDillonRepV1.txt",header=TRUE)
# delete subject 201t (same subject as 201t_2)
d1 <- subset(d1, d1$subject!='201t')
# rename subject 168bry and 201t_2
d1$subject <- ifelse(d1$subject=='168bry', '168b', as.character(d1$subject))
d1$subject <- ifelse(d1$subject=='201t_2', '201b', as.character(d1$subject))
d1$subject <- factor(d1$subject)
# edit subject ids such that they match with the working memory data
d1$subject <- unlist(strsplit(as.character(d1$subject), split=c('b')))
# remove 0s at id onset
d1$subject<-as.factor(as.integer(d1$subject))

d2 <- read.table("../data/dataDillonRepV2.txt",header=TRUE)
#summary(factor(d1$RESPONSE_ACCURACY)) 
#summary(factor(d2$RESPONSE_ACCURACY)) # 1=correct; 0=incorrect; -1=trial without a question
#xtabs(~RESPONSE_ACCURACY+item, d2)
# items followed by comprehension question
q <- sort(unique(subset(d2, RESPONSE_ACCURACY!=-1)$item))
# code absence of question as -1 in RESPONSE_ACCURACY of d1 (analoguously to coding in d2)
d1$RESPONSE_ACCURACY <- ifelse(!d1$item %in% q, -1, d1$RESPONSE_ACCURACY)

# Correct some wrong subject id labels in dataset V2 (=d2)
## 321_dr2 is the correct file for participant 321; 321_dr3 was incorrectly labeled and should have bene 386_dr3.
d2$subject <- factor(ifelse(d2$subject=='321_dr3', '386_dr3', as.character(d2$subject)))
##337_dr1 is the correct file for participant 337; 337_dr2 was incorrectly labeled and should be 338_dr1.
d2$subject <- factor(ifelse(d2$subject=='337_dr2', '338_dr1', as.character(d2$subject)))
# The data for participant 334 (from V2) is labeled as 223_dr6 => should be 334_dr6.
d2$subject <- factor(ifelse(d2$subject=='223_dr6', '334_dr6', as.character(d2$subject)))
# Data for participant 331 is labeled as 362_dr3 => should be 331_dr3.
d2$subject <- factor(ifelse(d2$subject=='2362_dr3', '331_dr3', as.character(d2$subject)))


d2$subject <- unlist(strsplit(as.character(d2$subject), split='[_dlr].*'))
d2$subject <- factor(d2$subject)


d <- rbind(d1,d2)
#colnames(d)
d$FPR <- as.numeric(as.logical(d$RBRC)) # first-pass regression
d$REG <- as.numeric(as.logical(d$TRC)) # whether or not there was an regression
d <- d[, c('subject', 'item', 'condition', 'RESPONSE_ACCURACY', 'roi', 'FFD','FPRT',  'FPR', 'RPD', 'RRT','TFT')]
colnames(d) <- c('subj', 'item', 'cond', 'acc', 'roi', 'FFD','FPRT',  'FPR', 'RPD', 'RRT','TFT')
d$item <- factor(d$item)
d$subj <- factor(d$subj)

#write.table(d, file='data/dataJMVV.txt', sep = '\t')
#d <- read.table(file='data/dataJMVV.txt', sep = '\t')
@

<<vars, include=TRUE, cache=TRUE, echo=FALSE, warning=TRUE, error=TRUE, message=TRUE>>=
Nsubj <- length(unique(factor(d$subj)))
#Nsubj_noWM <- length(which(is.na(op$span))) # number of subjects from which no wm data recorded
Nitem <- length(unique(factor(subset(d, cond!='filler')$item)))
Nfiller <- length(unique(factor(subset(d, cond=='filler')$item)))
@

<<accs, include=TRUE, cache=TRUE, echo=FALSE, warning=TRUE, error=TRUE, message=TRUE>>=
dq <- subset(d, item %in% q & roi==1) # comprehension accuracies
acc <- round(tapply(dq$acc, dq$cond, mean), 2)
# remove fillers
d <- subset(d, cond!='filler')
@

<<contrasts, include=TRUE, cache=TRUE,  echo=FALSE, warning=TRUE, error=TRUE, message=TRUE>>=
# Condition Labels (our labels are different from the ones used by Dillon et al 2013)
# a - d Agreement conditions.  
# a. grammatical, interference 
# b. grammatical, no interference
# c. ungrammatical, no interference
# d. ungrammatical, interference

# e - h Reflexives conditions. 
# e. grammatical, interference 
# f. grammatical, no interference 
# g. ungrammatical, no interference
# h. ungrammatical, interference


# Model 1: 
# main effect of dependency type (dep); positive effect => agr > refl
# main effect of grammaticality (gram);  positive effect => ungram > gram
# dep x gram
# interference within gram (int_gram); positive effect => inhib interference
# interference within ungram (int_ungram); => positive effect => inhib interference
# int_gram x dep; 
# int_ungram x dep; 
#### Only int_gram, int_ungram, int_gram x dep, int_ungram x dep are of theoretical interest


# Model 2: resolving interaction between dep and int_gram and between dep and int_ungram 
# main effect of dependency type (dep);  
# main effect of grammaticality (gram); 
# dep x gram
# interference within gram  in reflexives (int_gram_refl); positive effect => inhib interference
# interference within gram  in reflexives (int_gram_agr); positive effect => inhib interference
# interference within ungram in reflexives (int_ungram_refl); positive effect => inhib interference
# interference within ungram in agreement (int_ungram_agr); positive effect => inhib interference
#### Only int_gram_refl, int_gram_agr,  int_ungram_refl, int_ungram_agr are of theoretical interest

# Model 1wm: same as Model 1, with the following additional effects:
# main effect of working memory
# interactions between working memory and all other fixed effects (including interactions)

# Model 2wm: same as Model 2, with the following additional effects:
# main effect of working memory
# interactions between working memory and all other fixed effects (including interaction)


d$Dep <- ifelse(d$cond %in% c('a', 'b', 'c', 'd'), .5, -.5) # main effect of dependency type: agr=0.5, refl=-0.5
d$Gram <- ifelse(d$cond %in% c('a', 'b', 'e', 'f'), -.5, .5) # main effect of grammaticality: gram=-.5, ungram=.5
d$Int_gram <- ifelse(d$cond %in% c('a','e'), .5, ifelse(d$cond %in% c('b', 'f'), -.5, 0) ) # interference in grammatical sentences: distr-match=0.5, distr-mismatch=-0.5
d$Int_ungram <- ifelse(d$cond %in% c('d', 'h'), .5, ifelse(d$cond %in% c('c', 'g'), -.5, 0)) # interference in ungrammatical sentences: distr-match=0.5, distr-mismatch=-0.5
d$DepxInt_gram <-ifelse(d$cond %in% c('a', 'f'), .5, ifelse(d$cond %in% c('b', 'e'), -.5, 0))
d$DepxInt_ungram <- ifelse(d$cond %in% c('d', 'g'), .5, ifelse(d$cond %in% c('c', 'h'), -.5, 0))
d$DepxGram <- ifelse(d$cond %in% c('c', 'd', 'e', 'f'), .5, -.5)

d$DepxInt <- ifelse(d$cond %in% c('a', 'd', 'f', 'g'), 0.5, -0.5)
d$Int <- ifelse(d$cond %in% c('a', 'd', 'e', 'h'), 0.5, -0.5) # main effect of interference: int=0.5, no int=-0.5
d$GramxInt <- ifelse(d$cond %in% c('b', 'd', 'f', 'h'), 0.5, -0.5)
d$DepxGramxInt <- ifelse(d$cond %in% c('b', 'd', 'e', 'g'), 0.5, -0.5)

d$Int_gram_refl <- ifelse(d$cond %in% c('e'), .5, ifelse(d$cond %in% c('f'), -.5, 0))
d$Int_gram_agr <- ifelse(d$cond %in% c('a'), .5, ifelse(d$cond %in% c('b'), -.5, 0))
d$Int_ungram_refl <- ifelse(d$cond %in% c('h'), .5, ifelse(d$cond %in% c('g'), -.5, 0))
d$Int_ungram_agr <- ifelse(d$cond %in% c('d'), .5, ifelse(d$cond %in% c('c'), -.5, 0))
@

<<crit, include=TRUE, cache=TRUE, echo=FALSE, warning=TRUE, error=TRUE, message=TRUE>>=
crit <- subset(d, roi==12 & cond!='filler') 
@

<<analysesDillonrep,eval=FALSE,echo=FALSE,cache=TRUE>>=
mFFD <- lmer(FFD~ 1+Dep+Gram+DepxGram+Int_gram+Int_ungram+DepxInt_gram+DepxInt_ungram+ (1+DepxInt_ungram ||subj)+(1+DepxInt_ungram ||item),crit,control=lmerControl(calc.derivs=FALSE))
resFFD<-summary(mFFD)$coefficients[8,]

mFPRT<-lmer(FPRT~ 1+Dep+Gram+DepxGram+Int_gram+Int_ungram+DepxInt_gram+DepxInt_ungram+ (1+DepxInt_ungram ||subj)+(1+DepxInt_ungram ||item),crit,control=lmerControl(calc.derivs=FALSE))
resFPRT<-summary(mFPRT)$coefficients[8,]


mFPR<-glmer(FPR~ 1+Dep+Gram+DepxGram+Int_gram+Int_ungram+DepxInt_gram+DepxInt_ungram+ (1+DepxInt_ungram ||subj)+(1+DepxInt_ungram ||item),crit,family=binomial())
resFPR<-summary(mFPR)$coefficients[8,1:3]



mRPD<-lmer(RPD~ 1+Dep+Gram+DepxGram+Int_gram+Int_ungram+DepxInt_gram+DepxInt_ungram+ (1+DepxInt_ungram ||subj)+(1+DepxInt_ungram ||item),crit,control=lmerControl(calc.derivs=FALSE))
resRPD<-summary(mRPD)$coefficients[8,]

mRRT<-lmer(RRT~ 1+Dep+Gram+DepxGram+Int_gram+Int_ungram+DepxInt_gram+DepxInt_ungram+ (1+DepxInt_ungram ||subj)+(1+DepxInt_ungram ||item),crit,control=lmerControl(calc.derivs=FALSE))
resRRT<-summary(mRRT)$coefficients[8,]

mTFT<-lmer(TFT~ 1+Dep+Gram+DepxGram+Int_gram+Int_ungram+DepxInt_gram+DepxInt_ungram+ (1+DepxInt_ungram ||subj)+(1+DepxInt_ungram ||item),crit,control=lmerControl(calc.derivs=FALSE))

resTFT<-summary(mTFT)$coefficients[8,]

res<-rbind(resFFD,resFPRT,resFPR,resRPD,resRRT,resTFT)
res<-data.frame(DepVar=c("FFD","FPRT","FPR","RPD","RRT","TFT"),res)
@

<<xtableresrep,echo=FALSE,eval=FALSE>>=
xtable(res,include.rownames=FALSE)
@


\subsection{Differences between preregistration on open science platforms and Registered Reports}

Although preregistration without peer review is an effective way to reduce unconscious biases in one’s work, it has some potential disadvantages when compared to Registered Reports. Registered Reports offered by journals have the assurance of peer review and of guaranteed publication, once a preregistered manuscript has been accepted. By contrast, for preregistrations without peer review, publication is not guaranteed. The lack of peer review means that the preregistration of a study can be as thorough or as vague as the researcher deems appropriate. A vaguely specified research question and analysis plan can still allow for a plethora of statistical testing possibilities. Consequently, it is up to the scientific community to make preregistration a success or a failure: only a thoroughly implemented preregistration and a precisely followed research plan can reduce unconscious biases and help to separate confirmatory hypothesis tests from exploratory ones.


\subsection{Best practices for open science}
 
To ensure truly transparent research, in addition to using preregistration, it is essential to release all research materials, data and code along with the study publication. Releasing materials is a good first step toward allowing other researchers to attempt to replicate a study \parencite{open2015estimating}.

Publication of the data and analysis code is important so that the steps leading to a result can be fully retraced. Furthermore, it allows other researchers to conduct exploratory analyses which may help to formulate predictions for future (preregistered) studies, and to synthesize the available evidence on a particular topic \parencite{MahowaldEtAl2016}.


\section{Conclusion}

We have used a practical example to illustrate the usefulness of preregistration in cases where previous studies provide inconclusive evidence about the existence of a processing effect. While our example focused on native processing, bilingual processing research shows similar cases of inconclusive findings, such as the existence of a bilingual advantage in attentional systems \parencite{Bialystok2017,PaapEtAl2018}, the role of crosslinguistic influence in syntactic processing \parencite{DussiasDietrichVillegas2015}, and the decomposition of inflected forms during word recognition in native vs.\ non-native speakers \parencite{ClahsenVerissimo2016,FeldmanKroll2019}. 
Existing results in these domains could be used to preregister precise research questions including information about speaker groups, dependent measures and analysis plan(s) for expected effects. This process can counteract unconscious biases and can prevent the reporting of an exploratory result as a confirmatory one. 
We suggest that the hypothesis-driven research process should standardly include preregistrations, along with other open science practices such as releasing materials, data and code alongside publications to increase research transparency and reproducibility in bilingualism research.

\section{Acknowledgements}

This research was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) – project number 317633480 – SFB 1287, Project B03 (PIs: Shravan Vasishth and Ralf Engbert).


\printbibliography

\clearpage

\doublespacing

\end{document}


